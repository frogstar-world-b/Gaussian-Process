{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "Assume we have $\\color{blue}n$ observations of a response $\\color{blue}y$ and the corresponding values of input variable $\\color{blue}x$. \n",
    "\n",
    "$$\n",
    "\\color{blue}{\n",
    "x =\n",
    "\\begin{pmatrix}\n",
    "x_{1} \\\\\n",
    "x_{2} \\\\\n",
    "\\vdots  \\\\\n",
    "x_{n}\n",
    "\\end{pmatrix}}\n",
    "\\;\\;\n",
    "\\text{and}\n",
    "\\;\\;\n",
    "\\color{blue}{\n",
    "y =\n",
    "\\begin{pmatrix}\n",
    "y_{1} \\\\\n",
    "y_{2} \\\\\n",
    "\\vdots  \\\\\n",
    "y_{n}\n",
    "\\end{pmatrix}}\n",
    "\\;\\;\n",
    "$$\n",
    "\n",
    "If we use a Gaussian process (GP) model, then the vector of observations $\\color{blue}y$ has a multivariate normal distribution.\n",
    "\n",
    "Let us assume the GP has a constant mean $\\color{blue}{\\mu}$ and that we can generate a covariance matrix over $\\color{blue}x$ using a covariance function with parameters $\\color{blue}{\\sigma^2, \\ell}$. Typically we represent the covariance matrix as $\\color{blue}{\\Sigma(x,x)}$ or $\\color{blue}{\\Sigma_x}$.\n",
    "\n",
    "Thus, the probability distribution of $\\color{blue}y$ given $\\color{blue}x$ and the parameters $\\color{blue}{\\mu, \\sigma^2, \\ell}$ is given by,\n",
    "\n",
    "$$\n",
    "\\color{blue}{\n",
    "p(y|x, \\mu, \\sigma^2, \\ell) = \n",
    "(2\\pi)^{-\\frac{n}{2}} |\\Sigma_x|^{-\\frac{1}{2}} \\exp\\left(-\\frac{1}{2}\\left(y - \\mu \\right)^{T}\\Sigma^{-1}_x\\left(y - \\mu \\right) \\right)}\n",
    "$$\n",
    "\n",
    "One way to fit the parameters of this distribution by using the Maximum Likelihood Estimation (MLE) method. \n",
    "\n",
    "The likelihood has the same expression as the probability distribution above, but it treats the parameters as the unknown part of the function given the known values of $\\color{blue}{y}$ and $\\color{blue}{x}$. We write,\n",
    "$$\n",
    "\\color{blue}{\n",
    "\\mathcal{L}(\\mu, \\sigma^2, \\ell | y, x) = \n",
    "(2\\pi)^{-\\frac{n}{2}} |\\Sigma_x|^{-\\frac{1}{2}} \\exp\\left(-\\frac{1}{2}\\left(y - \\mu \\right)^{T}\\Sigma^{-1}_x\\left(y - \\mu \\right) \\right)}\n",
    "$$\n",
    "$$\n",
    "\\color{white}{\n",
    "\\mathcal{L}(\\mu, \\sigma^2, \\ell | y, x)}\\color{blue}{\n",
    "\\mathcal{L}(\\mu, \\sigma^2, \\ell | y, x)\\propto |\\Sigma_x|^{-\\frac{1}{2}} \\exp\\left(-\\frac{1}{2}\\left(y - \\mu \\right)^{T}\\Sigma^{-1}_x\\left(y - \\mu \\right) \\right)\n",
    "}\n",
    "$$\n",
    "\n",
    "\n",
    "Thus, the log likelihood is given by\n",
    "$$\n",
    "\\color{blue}{\n",
    "\\log \\mathcal{L}(\\mu, \\sigma^2, \\ell | y, x) \\propto \n",
    "-\\frac{1}{2}\\log|\\Sigma_x| - \\left(\\frac{1}{2}\\left(y - \\mu \\right)^{T}\\Sigma^{-1}_x\\left(y - \\mu \\right) \\right)}\n",
    "$$\n",
    "$$\n",
    "\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; \\;\\;    \n",
    "\\color{blue}{\\propto -\\log|\\Sigma_x| - \\left(y - \\mu \\right)^{T}\\Sigma^{-1}_x\\left(y - \\mu \\right)\n",
    "}\n",
    "$$\n",
    "\n",
    "An optimization routine, such as conjugate gradient optimization, could be used to obtain the MLEs.\n",
    "\n",
    "An alternative is to use a Bayesian framework by placing prior distributions on the hyper-parameters and obtaining their posterior distributions. \n",
    "\n",
    "\n",
    "\n",
    "### Prediction (aka interpolation)\n",
    "\n",
    "Assume that after we obtain the MLEs of the GP parameters, we want to obtain predictions using the fitted model over a new vector of input values $\\color{blue}{x^{pred}}$. Let $\\color{blue}{y^{pred}}$ represent the corresponding vector of predicted outputs.\n",
    "\n",
    "$$\n",
    "\\color{blue}{\n",
    "x^{pred} =\n",
    "\\begin{pmatrix}\n",
    "x_{1}^{pred} \\\\\n",
    "x_{2}^{pred} \\\\\n",
    "\\vdots  \\\\\n",
    "x_{m}^{pred}\n",
    "\\end{pmatrix}}\n",
    "\\;\\;\n",
    "\\text{and}\n",
    "\\;\\;\n",
    "\\color{blue}{\n",
    "y^{pred} =\n",
    "\\begin{pmatrix}\n",
    "y_{1}^{pred} \\\\\n",
    "y_{2}^{pred} \\\\\n",
    "\\vdots  \\\\\n",
    "y_{m}^{pred}\n",
    "\\end{pmatrix}}\n",
    "\\;\\;\n",
    "$$\n",
    "\n",
    "\n",
    "We can model the joint distribution of $\\color{blue}{y}$ and $\\color{blue}{y^{pred}}$ using this multivariate normal distribution and using the mean and covariance function of the GP,\n",
    "\n",
    "<br>\n",
    "$$\n",
    "\\color{blue}{\n",
    "\\begin{pmatrix}\n",
    "y \\\\\n",
    "y^{pred} \\\\\n",
    "\\end{pmatrix}\n",
    "\\sim\n",
    "\\mathcal N_{n+m} \\left( \n",
    "\\begin{pmatrix}\n",
    "\\,\n",
    "\\mu(x) \\\\\n",
    "\\mu\\left(x^{pred}\\right) \\\\\n",
    "\\end{pmatrix}\n",
    ",\n",
    "\\begin{pmatrix}\n",
    "\\Sigma(x, x) & \\Sigma(x, x^{pred})\\\\\n",
    "\\Sigma(x^{pred}, x) & \\Sigma(x^{pred}, x^{pred}) \\\\\n",
    "\\end{pmatrix}\n",
    "\\,\n",
    "\\right)\n",
    "}\n",
    "$$\n",
    "<br>\n",
    "\n",
    "What we have done is stacked $\\color{blue}{y}$ and $\\color{blue}{y^{pred}}$ into an $\\color{blue}{n+m}$ vector and modeled it as an $n+m$-variate normal. The mean vector is also $\\color{blue}{n+m}$ and the covariance matrix is $\\color{blue}{(n+m)\\times (n+m)}$. \n",
    "\n",
    "- The $\\color{blue}{n\\times 1}$ subvector $\\color{blue}{\\mu(x)}$ includes the elements of the mean vector over $\\color{blue}{x}$.\n",
    "- The $\\color{blue}{m\\times 1}$ subvector $\\color{blue}{\\mu(x^{pred})}$ includes the elements of the mean vector over $\\color{blue}{x^{pred}}$.\n",
    "- The $\\color{blue}{n\\times n}$ submatrix $\\color{blue}{\\Sigma(x, x)}$ is the covariance matrix over $\\color{blue}{x}$.\n",
    "- The $\\color{blue}{n\\times m}$ submatrix $\\color{blue}{\\Sigma(x, x^{pred})}$ is the \"cross-covariance\" matrix between $\\color{blue}{x}$ and $\\color{blue}{x^{pred}}$.\n",
    "- The $\\color{blue}{m\\times n}$ submatrix $\\color{blue}{\\Sigma(x^{pred}, x)}$ is the transpose of $\\color{blue}{\\Sigma(x, x^{pred})}$.\n",
    "- The $\\color{blue}{m\\times m}$ submatrix $\\color{blue}{\\Sigma(x^{pred}, x^{pred})}$ is the covariance matrix over $\\color{blue}{x^{pred}}$.\n",
    "\n",
    "\n",
    "Using the properties of the multivariate normal probability distribution, we can obtain the conditional distribution $\\color{blue}{y^{pred}| y, \\mu ,\\sigma^2, \\ell}$, which will be a multivariate normal with mean (expectation) $\\color{blue}{\\mathbb E}$, and covariance $\\color{blue}{\\mathbb C}$ given by\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\color{blue}{\\mathbb E = \\mu\\left(x^{pred}\\right) + \\Sigma(x^{pred}, x)\\Sigma(x, x)^{-1}\\left[y - \\mu(x)\\right]}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\color{blue}{\\mathbb C = \\Sigma(x^{pred}, x^{pred}) - \\Sigma(x^{pred}, x)\\,\\Sigma(x, x)^{-1}\\, \\Sigma(x, x^{pred})}\n",
    "$$\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
